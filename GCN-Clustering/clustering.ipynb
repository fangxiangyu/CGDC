{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use knn to construct graph ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "from spektral.datasets.citation import Cora\n",
    "from spektral.layers.convolutional import GCSConv\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "from spektral.layers.pooling import MinCutPool\n",
    "from spektral.utils.convolution import normalized_adjacency\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from scipy import sparse\n",
    "\n",
    "def load_data():\n",
    "    features = np.load('./data/cifar10-features.npy')\n",
    "    \n",
    "    labels = np.load('./data/cifar10-labels.npy')\n",
    "   \n",
    "    import faiss\n",
    "    \n",
    "    n, dim = features.shape[0], features.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index = faiss.index_cpu_to_all_gpus(index)\n",
    "    index.add(features)\n",
    "    distances, indices = index.search(features, 26)\n",
    "    \n",
    "    link = np.reshape(distances[:,1:],(-1,1))\n",
    "    link = link.squeeze(1)\n",
    "    topk = indices\n",
    "    edges = []\n",
    "    for i in range(len(topk)):\n",
    "\n",
    "        for j in range(1,len(topk[0])):\n",
    "            tmp = [i]\n",
    "            tmp.append(topk[i][j]) \n",
    "            edges.append(tmp)\n",
    "    edges = np.array(edges)        \n",
    "    adj = sp.coo_matrix((link, (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(topk.shape[0], topk.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return adj,features,labels\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    mx = mx.astype(np.float32)\n",
    "    return mx\n",
    "\n",
    "def _hungarian_match(flat_preds, flat_targets, preds_k, targets_k):\n",
    "    # Based on implementation from IIC\n",
    "    num_samples = flat_targets.shape[0]\n",
    "\n",
    "    assert (preds_k == targets_k)  # one to one\n",
    "    num_k = preds_k\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "\n",
    "    for c1 in range(num_k):\n",
    "        for c2 in range(num_k):\n",
    "            # elementwise, so each sample contributes once\n",
    "            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "\n",
    "    # num_correct is small\n",
    "    match = linear_sum_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "\n",
    "    # return as list of tuples, out_c to gt_c\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,X,y = load_data()\n",
    "\n",
    "n_clusters = y.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, S_pool = model(inputs, training=True)\n",
    "        loss = sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return model.losses[0], model.losses[1], S_pool\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn import metrics\n",
    "\n",
    "log_dir = './result_cifar10/logdir/'\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "np.random.seed(1)\n",
    "epochs = 5000  # Training iterations\n",
    "lr = 5e-4  # Learning rate\n",
    "\n",
    "################################################################################\n",
    "# LOAD DATASET\n",
    "################################################################################\n",
    "#dataset = Cora()\n",
    "#adj, x, y = dataset[0].a, dataset[0].x, dataset[0].y\n",
    "\n",
    "a_norm = normalized_adjacency(A)\n",
    "a_norm = sp_matrix_to_sp_tensor(a_norm)\n",
    "F = X.shape[-1]\n",
    "#y = np.argmax(y, axis=-1)\n",
    "n_clusters = y.max() + 1\n",
    "\n",
    "################################################################################\n",
    "# MODEL\n",
    "################################################################################\n",
    "x_in = Input(shape=(F,), name=\"X_in\",dtype='float32')\n",
    "a_in = Input(shape=(None,), name=\"A_in\", sparse=True,dtype='float32')\n",
    "\n",
    "x_0 = GCSConv(32, activation=\"elu\")([x_in, a_in])\n",
    "x_1 = GCSConv(16, activation=\"elu\")([x_0, a_in])\n",
    "x_1, a_1, s_1 = MinCutPool(n_clusters, return_mask=True)([x_1, a_in])\n",
    "\n",
    "model = Model([x_in, a_in], [x_1, s_1])\n",
    "\n",
    "################################################################################\n",
    "# TRAINING\n",
    "################################################################################\n",
    "# Setup\n",
    "inputs = [X, a_norm]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Fit model\n",
    "loss_history = []\n",
    "nmi_history = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    outs = train_step(inputs)\n",
    "    outs = [o.numpy() for o in outs]\n",
    "    loss_history.append((outs[0], outs[1], (outs[0] + outs[1])))\n",
    "    c = np.argmax(outs[2], axis=-1)\n",
    "    #nmi_history.append(v_measure_score(y,c))\n",
    "    num_elems = y.shape[0]\n",
    "    match = _hungarian_match(c,y,10,10)\n",
    "    reordered_preds = np.zeros(num_elems, dtype=c.dtype)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[c == int(pred_i)] = int(target_i)\n",
    "\n",
    "    acc = int((reordered_preds == y).sum()) / float(num_elems)\n",
    "    nmi = metrics.normalized_mutual_info_score(y, c)\n",
    "    ari = metrics.adjusted_rand_score(y, c)\n",
    "    \n",
    "    writer.add_scalar('train_loss',outs[0]+outs[1],epoch)\n",
    "    writer.add_scalar('acc',acc,epoch)\n",
    "    writer.add_scalar('nmi',nmi,epoch)\n",
    "    writer.add_scalar('ari',nmi,epoch)\n",
    "loss_history = np.array(loss_history)\n",
    "\n",
    "################################################################################\n",
    "# RESULTS\n",
    "################################################################################\n",
    "_, s_out = model(inputs, training=False)\n",
    "s_out = np.argmax(s_out, axis=-1)\n",
    "print(acc)\n",
    "print(nmi)\n",
    "print(ari)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}